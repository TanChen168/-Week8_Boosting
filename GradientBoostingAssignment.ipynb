{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GradientBoostingAssignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TanChen168/Week8_Boosting/blob/main/GradientBoostingAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1a1R3UyYQMH"
      },
      "source": [
        "# Intro to Gradient Boosting\n",
        "\n",
        "![gradient boosting image](https://media.geeksforgeeks.org/wp-content/uploads/20200721214745/gradientboosting.PNG)\n",
        "\n",
        "Image thanks to [Geeks for Geeks](https://www.geeksforgeeks.org/ml-gradient-boosting/)\n",
        "\n",
        "In this assignment you will:\n",
        "1. import and prepare a dataset for modeling\n",
        "2. test and evaluate 3 different boosting models and compare the fit times of each.\n",
        "3. tune the hyperparameters of the best model to reduce overfitting and improve performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBHKlzCubQOq"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import make_column_selector, make_column_transformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report, plot_confusion_matrix\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#import accuracy, precision, recall, classification report, and confusion matrix scoring functions\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1guzybQJjbn6"
      },
      "source": [
        "In this assignment you will be working with census data.  Your goal is to predict whether a person will make more or less than $50k per year in income.\n",
        "\n",
        "The data is available [here](https://drive.google.com/file/d/1drlRzq-lIY7rxQnvv_3fsxfIfLsjQ4A-/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsllcFvrb7YC"
      },
      "source": [
        "# Get data\n",
        "df = pd.read_csv('/content/sample_data/census_income.csv')\n",
        "df = df.drop(columns='Unnamed: 0')\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6_1E6psj1J0"
      },
      "source": [
        "Prepare your dataset for modeling.\n",
        "\n",
        "Remember to: \n",
        "1. Check for missing data, bad data, and duplicates.\n",
        "2. Check your target class balance.\n",
        "3. Perform your validation split\n",
        "4. Create a preprocessing pipeline to use with your models.\n",
        "5. Fit and evaluate your models using pipelines"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up bad data\n",
        "df = df[df.workclass != '?']\n",
        "df.info()"
      ],
      "metadata": {
        "id": "CTsTVGFpsNNN",
        "outputId": "cd68dce9-0f43-4a15-9548-9e23daacde90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 30725 entries, 0 to 32560\n",
            "Data columns (total 13 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   age             30725 non-null  int64 \n",
            " 1   workclass       30725 non-null  object\n",
            " 2   education       30725 non-null  object\n",
            " 3   marital-status  30725 non-null  object\n",
            " 4   occupation      30725 non-null  object\n",
            " 5   relationship    30725 non-null  object\n",
            " 6   race            30725 non-null  object\n",
            " 7   sex             30725 non-null  object\n",
            " 8   capital-gain    30725 non-null  int64 \n",
            " 9   capital-loss    30725 non-null  int64 \n",
            " 10  hours-per-week  30725 non-null  int64 \n",
            " 11  native-country  30725 non-null  object\n",
            " 12  income-class    30725 non-null  object\n",
            "dtypes: int64(4), object(9)\n",
            "memory usage: 3.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create feature and target dataset\n",
        "X = df.drop(columns = 'income-class')\n",
        "# Encode our target\n",
        "y = df['income-class']"
      ],
      "metadata": {
        "id": "Hg9CTBV-skPG"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check target class balance\n",
        "y.value_counts()"
      ],
      "metadata": {
        "id": "dVg2Xr8etsL8",
        "outputId": "0a9d5400-80f3-4944-b550-abd3abfd102c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<=50K    23075\n",
              ">50K      7650\n",
              "Name: income-class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-Wh3RssgPAu"
      },
      "source": [
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare and create pipeline\n",
        "\n",
        "# Initialize transformers\n",
        "num_selector = make_column_selector(dtype_include='number')\n",
        "cat_selector = make_column_selector(dtype_include='object')\n",
        "mean_imputer = SimpleImputer(strategy='mean')\n",
        "freq_imputer = SimpleImputer(strategy='most_frequent')\n",
        "scaler = StandardScaler()\n",
        "ohe_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "\n",
        "# This is the pipeline for numeric columns\n",
        "num_pipe = make_pipeline(mean_imputer, scaler)\n",
        "# This is the pipeline for categorical columns\n",
        "cat_pipe = make_pipeline(freq_imputer, ohe_encoder)\n",
        "\n",
        "#Column matching \n",
        "num_tuple = (num_pipe, num_selector)\n",
        "cat_tuple = (cat_pipe, cat_selector)\n",
        "column_transformer = make_column_transformer(num_tuple, cat_tuple)\n",
        "\n",
        "column_transformer.fit(X_train)\n",
        "\n",
        "X_train_processed = column_transformer.transform(X_train)\n",
        "X_test_processed = column_transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "7jAmza9tvYKw"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOIAxdz7k7d_"
      },
      "source": [
        "# eXtreme Gradient Boosting\n",
        "We are going to compare both metrics and fit times for our models.  Notice the 'cell magic' in the top of the cell below.  By putting `%%time` at the top of a notebook cell, we can tell it to output how long that cell took to run.  We can use this to compare the speed of each of our different models.  Fit times can be very important for models in deployment, especially with very large dataset and/or many features.\n",
        "\n",
        "Instantiate an eXtreme Gradient Boosting Classifier (XGBClassifier) below, fit it, and print out a classification report.  Take note of the accuracy, recall, precision, and f1-score, as well as the run time of the cell to compare to our next models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du_JvXzBgHcU",
        "outputId": "578c551e-9a53-4bfb-af8d-384795639831",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "xgb = XGBClassifier()\n",
        "xgb.fit(X_train_processed, y_train)\n",
        "xgbPredictions = xgb.predict(X_test_processed)\n",
        "\n",
        "accuracy = accuracy_score(y_test, xgbPredictions)\n",
        "# recall = recall_score(y_test, xgbPredictions)\n",
        "# precisionscore = precision_score(y_test, xgbPredictions)\n",
        "\n",
        "\n",
        "# print('Training accuracy:', xgb.score(X_train_processed, y_train))\n",
        "# print('Testing accuracy:', xgb.score(X_test_processed, y_test))\n",
        "# print('Accuracy: ',  accuracy)\n",
        "# print('Recall: ', recall)\n",
        "# print('Precision Score: ', precisionscore)\n",
        "\n",
        "Creport = classification_report(y_test, xgbPredictions)\n",
        "print(Creport)\n",
        "\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.88      0.95      0.91      5798\n",
            "        >50K       0.80      0.60      0.68      1884\n",
            "\n",
            "    accuracy                           0.86      7682\n",
            "   macro avg       0.84      0.77      0.80      7682\n",
            "weighted avg       0.86      0.86      0.86      7682\n",
            "\n",
            "CPU times: user 5.87 s, sys: 26.7 ms, total: 5.9 s\n",
            "Wall time: 5.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DdMdPlLgrVm"
      },
      "source": [
        "Which target class is your model better at predicting?  Is it significantly overfit?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWu_4o2nm-HP"
      },
      "source": [
        "# More Gradient Boosting\n",
        "\n",
        "Now fit and evaluate a Light Gradient Boosting Machine and a the Scikit Learn (sklearn) gradient boost model.  Remember to use the `%%time` cell magic command to get the run time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpRP2a_UlTYX"
      },
      "source": [
        "## LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKxj1YCloRML",
        "outputId": "b3730749-6168-40d8-8a01-fefbbf7b48ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "lgbm = LGBMClassifier(max_depth = 6)\n",
        "lgbm.fit(X_train_processed, y_train)\n",
        "lgbmPredictions = lgbm.predict(X_test_processed)\n",
        "\n",
        "accuracy = accuracy_score(y_test, lgbmPredictions)\n",
        "Creport = classification_report(y_test, lgbmPredictions)\n",
        "print(Creport)\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.89      0.94      0.92      5798\n",
            "        >50K       0.79      0.65      0.71      1884\n",
            "\n",
            "    accuracy                           0.87      7682\n",
            "   macro avg       0.84      0.80      0.81      7682\n",
            "weighted avg       0.87      0.87      0.87      7682\n",
            "\n",
            "CPU times: user 1.12 s, sys: 11.5 ms, total: 1.14 s\n",
            "Wall time: 1.24 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iT6rGsjlWfC"
      },
      "source": [
        "## GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_zcJ2pVoSUr",
        "outputId": "ac164e20-dfbf-45b7-dcd4-f3f0e7ed8aa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "gbc = GradientBoostingClassifier()\n",
        "gbc.fit(X_train_processed, y_train)\n",
        "gbcmPredictions = lgbm.predict(X_test_processed)\n",
        "\n",
        "accuracy = accuracy_score(y_test, gbcmPredictions)\n",
        "Creport = classification_report(y_test, gbcmPredictions)\n",
        "print(Creport)\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.90      0.94      0.92      5798\n",
            "        >50K       0.78      0.67      0.72      1884\n",
            "\n",
            "    accuracy                           0.87      7682\n",
            "   macro avg       0.84      0.81      0.82      7682\n",
            "weighted avg       0.87      0.87      0.87      7682\n",
            "\n",
            "CPU times: user 8.25 s, sys: 7.39 ms, total: 8.25 s\n",
            "Wall time: 8.27 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSvJ4frz4Q0P"
      },
      "source": [
        ""
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9VNanqkoTf7"
      },
      "source": [
        "\n",
        "# Tuning Gradient Boosting Models\n",
        "\n",
        "Tree-based gradient boosting models have a LOT of hyperparameters to tune.  Here are the documentation pages for each of the 3 models you used today:\n",
        "\n",
        "1. [XGBoost Hyperparameter Documentation](https://xgboost.readthedocs.io/en/latest/parameter.html)\n",
        "2. [LightGBM Hyperparameter Documentation](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)\n",
        "3. [Scikit-learn Gradient Boosting Classifier Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
        "\n",
        "Choose the model you felt performed the best when comparing multiple metrics and the runtime for fitting, and use GridSearchCV to try at least 2 different values each for 3 different hyper parameters in boosting model you chose.\n",
        "\n",
        "See if you can create a model with an accuracy between 86 and 90.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3M-SZpN_7iN"
      },
      "source": [
        "import time\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "lgbm = LGBMClassifier()\n",
        "\n",
        "parameters = {'num_leaves':[20, 60,100], 'min_child_samples':[5,10, 15],'max_depth':[5, 10,20]}\n",
        "\n",
        "#Define the scoring\n",
        "clf=GridSearchCV(lgbm,parameters,scoring='accuracy')\n",
        "clf.fit(X=X_train_processed, y=y_train)\n",
        "print(clf.best_params_)\n",
        "predicted=clf.predict(X_test_processed)\n",
        "print('Classification of the result is:')\n",
        "print(accuracy_score(y_test, predicted))\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print('Execution time is:')\n",
        "print(end - start)\n",
        "\n",
        "# lgbm.fit(X_train_processed, y_train)\n",
        "# lgbmPredictions = lgbm.predict(X_test_processed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAV4kPhjHGf5"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Evaluate your model using a classifiation report and/or a confusion matrix.  Explain in text how your model performed in terms of precision, recall, and it's ability to predict each of the two classes.  Also talk about the benefits or drawbacks of the computation time of that model.\n",
        "\n",
        "-- The classification report using the tuned hyperparameters performed slightly better than the manual fitting of LGBM (including precision and recall scoring). The computing time by itself is far better compare to XGB and GB model but except it takes time to run through all defined parameters with GridSearchCV. training time also increase if more parameters and ranges were added."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMQkGSUeEyEx",
        "outputId": "c7930b31-f0e9-4010-c2cb-df0d6649ea37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lgbm = LGBMClassifier(max_depth = 10, min_child_samples=10, num_leaves=20)\n",
        "lgbm.fit(X_train_processed, y_train)\n",
        "lgbmPredictions = lgbm.predict(X_test_processed)\n",
        "\n",
        "accuracy = accuracy_score(y_test, lgbmPredictions)\n",
        "Creport = classification_report(y_test, lgbmPredictions)\n",
        "print(Creport)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.90      0.94      0.92      5798\n",
            "        >50K       0.79      0.67      0.72      1884\n",
            "\n",
            "    accuracy                           0.87      7682\n",
            "   macro avg       0.84      0.80      0.82      7682\n",
            "weighted avg       0.87      0.87      0.87      7682\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjtsxJQ5l7Hi"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this assignment you practiced:\n",
        "1. data cleaning\n",
        "2. instantiating, fitting, and evaluating boosting models using multiple metrics\n",
        "3. timing how long it takes a model to fit and comparing run times between multiple models\n",
        "4. and choosing a final model based on multiple metrics.\n",
        "\n"
      ]
    }
  ]
}